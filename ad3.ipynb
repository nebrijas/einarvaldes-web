{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa848dfc",
   "metadata": {},
   "source": [
    "# AD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef46de",
   "metadata": {},
   "source": [
    "Esta es la actividad dirigida 3 que consiste en hacer un ejercicio de programación literaria, aprovechando el código que hemos usado en programación con Python donde realizamos *web scraping*.\n",
    "\n",
    "A continuación pongo el código fuente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38c1af",
   "metadata": {},
   "source": [
    "## Código fuente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe86430",
   "metadata": {},
   "source": [
    "El código que colocaremos a continuación lo vamos a transformar en progamación literaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b43053",
   "metadata": {},
   "source": [
    "``` python\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    " \n",
    "resultados = []\n",
    " \n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    " \n",
    "tags = soup.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    " \n",
    "tags = soup2.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    " \n",
    "tags = soup3.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    " \n",
    "tags = soup4.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    " \n",
    "tags = soup5.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    " \n",
    "tags = soup6.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    " \n",
    "tags = soup7.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    " \n",
    "tags = soup8.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    " \n",
    "tags = soup9.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    " \n",
    "tags = soup10.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    " \n",
    "tags = soup11.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    " \n",
    "tags = soup12.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    " \n",
    "tags = soup13.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    " \n",
    "tags = soup14.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    " \n",
    "tags = soup15.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    " \n",
    "tags = soup16.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    " \n",
    "tags = soup17.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    " \n",
    "os.system(\"clear\")\n",
    " \n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38f8c0",
   "metadata": {},
   "source": [
    "## Ejercicio de Programación Literaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb4568",
   "metadata": {},
   "source": [
    "### Librerías\n",
    "\n",
    "Vamos a utilizar las siguientes librerías y módulos\n",
    "\n",
    "#### Módulos del sistema\n",
    "\n",
    "- [time](https://docs.python.org/3/library/time.html) es una biblioteca de Python pequeña y minimalista para manejar las conversiones de tiempo hacia y desde zonas horarias, de una vez por todas.\n",
    "- [csv](https://docs.python.org/3/library/csv.html) es una librería de lectura y escritura de archivos. \n",
    "- [re](https://docs.python.org/es/3/library/re.html) Este módulo proporciona operaciones de coincidencia de expresiones regulares similares a las encontradas en Perl.\n",
    "- [os](https://docs.python.org/es/3.10/library/os.html) provee una manera versátil de usar funcionalidades dependientes del sistema operativo.\n",
    "\n",
    "#### Librerías externas\n",
    "\n",
    "- [requests](https://requests.readthedocs.io/en/latest/) es una biblioteca ``HTTP`` simple pero elegante.\n",
    "- [bs4](https://pypi.org/project/beautifulsoup4/) es una biblioteca que facilita extraer información de páginas web.\n",
    "- [pandas](https://pypi.org/project/pandas/) es un paquete de Python que proporciona estructuras de datos rápidas, flexibles y expresivas diseñadas para que trabajar con datos \"relacionales\" o \"etiquetados\" sea fácil e intuitivo.\n",
    "- [termcolor](https://pypi.org/project/termcolor/) permite la impresión de texto coloreado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea687f",
   "metadata": {},
   "source": [
    "## Instalación de librerías\n",
    "\n",
    "Las librerías que vienen ton Python no hay que instalarlas, pero las otras sí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d65c3",
   "metadata": {},
   "source": [
    "``pip install requests bs4 pandas termcolor``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e82767",
   "metadata": {},
   "source": [
    "Para el proceso de importar librerías, se pueden hacer de distintas maneras:\n",
    "   - Algunas se pueden importar directamente, como es el caso de ``requests``.\n",
    "   - Otras, por ejemplo, se importan cambiando el nombre que se empleará para invocarlar, como es el caso de ``pandas``.\n",
    "   - También está el caso de ``bs4`` y ``termcolor`` que se importan con determinados componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe1001",
   "metadata": {},
   "source": [
    "## Descripción del ejercicio con el código Python\n",
    "\n",
    "El primer paso que se debe aplicar para la ejecución de este código se debe importar la librería necesarias para el guion. Las mostramos a continuación: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2113ef3e",
   "metadata": {},
   "source": [
    "``` python \n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da0f4f",
   "metadata": {},
   "source": [
    "Luego se debe colocar la variable ```resultados = []```. Ahí se almacenarán los resultados de las URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e4c9f",
   "metadata": {},
   "source": [
    "```python \n",
    "resultados = []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbec7f8",
   "metadata": {},
   "source": [
    "## Extracción de resultados a través de las URLs \n",
    "\n",
    "Se realiza una petición **HTTP GET** a la **URL** de la web donde se quiere hacer *web scraping*. Esta petición devuelve una respuesta que se almacena en una variable llamada ```req```. Esta variable es de tipo objeto y contiene tanto atributos como funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdde6c",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7cd096",
   "metadata": {},
   "source": [
    "En el objeto ```req``` see utiliza el atributo ```status_code``` para comprobar la petición se ha realizado con éxito. Si el valor es igual a 200, la petición se ha realizado con éxito. En caso contrario, se lanza una excepción para avisar cual ha sido la URL que ha fallado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc3349",
   "metadata": {},
   "source": [
    "```python \n",
    "if (req.status_code != 200):\n",
    "    raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059fb5e8",
   "metadata": {},
   "source": [
    "Se extrae todo el texto HTML de la página web con ayuda de una de las funciones de la librería **bs4**: ```BeautifulSoup```. Este paso se realiza cuando secomprueba que la petición ha sido exitosa. Con esta función se le pasan dos parámetros: el texto obtenido de la petición **HTTP** a la web, y el tipo de texto que tiene que extraer. El resultado se almacena en una variable llamada ```soup```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbc554",
   "metadata": {},
   "source": [
    "``` python\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096d79e4",
   "metadata": {},
   "source": [
    "Solo es necesario el texto de las etiquetas, ya que el objetivo es obtener los titulares de cada **URL**. Para hacer el filtrado, se utiliza una de las funciones que contiene el objeto ```soup``` llamada *findAll*. Con esta función solo se almacenan en la variable ```tags``` el texto de los titulares de la web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2be31",
   "metadata": {},
   "source": [
    "``` python\n",
    "tags = soup.findAll(\"h2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845eed17",
   "metadata": {},
   "source": [
    "En la variable ```tags``` se almacena todos los titulares obtenidos. Para guardarlos de forma individual en la variable *resultados*, se recorren con una sentencia **for** y se añaden al final en cada iteración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7831b",
   "metadata": {},
   "source": [
    "```python\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af304fcb",
   "metadata": {},
   "source": [
    "Este procedimiento se repite el número de veces que sean necesarias para la extracción de los titulares de las didiferentes *URLs*. A continuación colocaremos la codificación ***Python*** para ejemplificarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58157d2e",
   "metadata": {},
   "source": [
    "```python\n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    "\n",
    "tags = soup2.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    "\n",
    "tags = soup3.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    "\n",
    "tags = soup4.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    "\n",
    "tags = soup5.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    "\n",
    "tags = soup6.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    "\n",
    "tags = soup7.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    "\n",
    "tags = soup8.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    "\n",
    "tags = soup9.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    "\n",
    "tags = soup10.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    "\n",
    "tags = soup11.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    "\n",
    "tags = soup12.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    "\n",
    "tags = soup13.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    "\n",
    "tags = soup14.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    "\n",
    "tags = soup15.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    "\n",
    "tags = soup16.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    "\n",
    "tags = soup17.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb7380",
   "metadata": {},
   "source": [
    "Es importante mencionar que se utiliza el comando ```clear``` antes de mostrar los resultados por la termial para que la consola quede limpia y se vean bien los titulares extraídos con el código ***Python***.\n",
    "```python\n",
    "\n",
    "os.system(\"clear\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818390f",
   "metadata": {},
   "source": [
    "Las categorías por las que se van a clasificar los titulares extraídos se muestran por consola con el siguiente paso:\n",
    "```python\n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fe675",
   "metadata": {},
   "source": [
    "## Titulares extraídos por categorías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283319e4",
   "metadata": {},
   "source": [
    "Se muestra el título de la categoría en la consola.\n",
    "\n",
    "```python\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce885a1",
   "metadata": {},
   "source": [
    "El bucle ```for``` se utiliza para filtrar los titulares por categoría. Este en cada interacción comprueba si el titular contiene la palabra clave. De ser así, queda guardado el titular en la variable ```str_match```.\n",
    "\n",
    "```python\n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d064612",
   "metadata": {},
   "source": [
    "Finalmente, se muestran todos los titulares de la categoría en concreto.\n",
    "```python \n",
    "\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46940af9",
   "metadata": {},
   "source": [
    "Tal cual como en un procedimiento aplicado anteriormente, este último bloque de código se repite tantas veces como categorías se definan.\n",
    "```python\n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a3e43",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "El código fuente creado por la compañera Iria Santos buscaba realizar un *web scraping* de algunas categorías dentro del sitio web del medio **El País**, de España. A través de la instrucción dada por el profesor de **Programación** del **Máster en Periodismo Digital y de Datos**, debíamos encontrar títulares específicos con palabras como: feminismo, mujer, homicidios y machismo.\n",
    "\n",
    "No obstante, fue un trabajo hecho en grupos de cuatro personas y cada grupo tenía la libertad de aplicar el ejercicio a su estilo. Utilizando las herramientas que brinda ***Python***, podíamos extraer la información que quisiéramos. Lo que deseaba comprobar es que todos los alumnos comprendieran el procedimiento para la extracción de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
